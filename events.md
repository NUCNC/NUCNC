---
layout: page
title: Events
---
# Due to the pandemic events are being held on Zoom. Join the slack channel or sign up to our listserv for Zoom meeting details.

## Machine Learning Basics: How do Computers See?
**Moderator: Nathan Whitmore** <br>
**November 12, 2020 13:30 PM (Zoom)** <br>
Over the last decade, computer image recognition has risen to rival human vision in applications from from unlocking your phone to detecting tumors in CT scans. The key to this success has been convolutional neural networks (CNNs), which are simple yet incredibly powerful machine learning systems. This meeting will focus on developing an intuitive understanding of how these networks work, and how you can use them to detect patterns in both image and non-image data. <br>
Optional reading: https://towardsdatascience.com/simple-introduction-to-convolutional-neural-networks-cdf8d3077bac

## Journal Club Discussion
**Moderators: Maite Azcorra and Vivek Sagar** <br>
**November 26, 2020 12:30 PM (Zoom)** <br>
1. On the Integration of Space, Time, and Memory – Eichenbaum, Neuron 2017 https://www.sciencedirect.com/science/article/pii/S0896627317305603 <br>

2. High-amplitude co-fluctuations in cortical activity drive resting-state functional connectivity – PNAS 2020 https://www.pnas.org/content/early/2020/10/21/2005531117

## Book Club Discussion
**Moderator: Maggie Swerdloff** <br>
**December 10, 2020 12:30 PM (Zoom)** <br>
Ignorance: How it Drives Science - Stuart Firestein


# Past Events
Please see the content page for resources.

# January 27, 2020 5 PM (Wieboldt 421, Telec. to Tech L251)
**Revisiting Linear Algebra, Vivek Sagar**<br>
Has reading up on eigenvalue problems been on your to-do list for over a year? Do you simply pretend that you understand things when your coworker is talking about matrix diagonalization or singular value decomposition in the lab meeting? Do you now realize that PCA is cool but wish you had paid more attention in the linear algebra class in college? Or perhaps you are just looking for a safe, non-judgmental space to let your inner mathematician out. In any case, join us for the first NUCNC event of this decade – “Understanding eigenvalue problems”. I will remind you of what eigenvalue problems are, where they can be used and how to solve them without a computer. We will also discuss how engineers use linear algebra to solve complicated problems of the universe, physicists use it to visualize the laws of the universe and mathematicians use it to create a universe of their own. <br>
Pre-requisites: Just know how to multiply matrices and what a matrix determinant and inverse is.<br>

# November 4, 2019 5 PM (Tech L251, Telec. to Wieboldt 421)
**Featured Faculty Talk: Human Brain Networks and Hubs, Dr. Caterina Gratton**<br>
The human brain is organized into large-scale networks, or systems, of interacting brain regions. These interactions can be measured in humans with functional Magnetic Resonance Imaging (fMRI), by measuring correlations in the patterns of activity between different regions. Increasingly sophisticated techniques enable the mapping of brain networks at unprecedented levels of detail, but many questions still remain. In this presentation, I will tackle two recent studies that we have undertaken to better understand human brain networks and their contributions to brain function. In the first study, we examine whether the topology of brain networks – specifically, the presence of hub regions – is important for brain function, by examining the consequences of damage to these regions. In the second study, we examine the variability in brain networks within and across subjects at different time-scales. Jointly, these studies suggest that network topology has important implications for human brain function, and that measures of network organization are stable features that can be used to measure trait-like variability in brain organization. I will close with a pointer to a tutorial demonstrating four useful visualizations of functional networks.<br>

# September 9, 2019 5 PM (Pancoe 4113, Telec. to Wieboldt 421)
**The Shift from Life in Water to Life on Land Advantaged Planning in Visually-Guided Behavior, Ugurcan Mugan**<br>
Studies of animal decision making reveal two distinct, competing control paradigms: habit- and plan-based action selection. The habit system has largely been associated with the lateral striatum and its dopaminergic afferents. Conserved basal ganglia structure from lamprey to mammals suggests that habitual control evolved early and persisted through vertebrate evolution. On the other hand, the planning system requires the interaction between the hippocampus and the prefrontal cortex or its homolog in birds. While the hippocampus (and its homologs) exists in both aquatic and land vertebrates, there seems to be no known homolog of the PFC in non-mammalian aquatic vertebrates.<br>
Our prior research into the water-to-land-transition indicates large increases in both visual range and observed environmental complexity. We hypothesize that these changes advantaged neural structures promoting planning over habit. To test this hypothesis, we developed two simulations of predator-prey dynamics. First, we simulated aquatic conditions in which the prey’s visual range was varied in a simple environment. Second, we simulated terrestrial conditions in which we varied environmental complexity by adding clutter and extended the prey visual range to the whole environment, except as blocked by occlusions. In both simulations, the prey was configured to have either habit-based action selection, or plan-based action selection with a preset number of states it could forward simulate. Our aquatic simulations strongly suggest that planning, while advantaged in proportion to visual range, cannot improve performance over habit in simple environments. In line with this idea, the results of our terrestrial simulations indicate that in spatially simple environments (near-open and highly cluttered) planning is not advantaged over habit. In contrast, we find that in spatially complex environments at midrange clutter level, planning produces complex predator avoidance behaviors and significantly increases performance over habit. Moreover, these results suggest that forward shifts in neural representations and the modulation of theta power in the mPFC during spatial navigation may be related to the distribution of cell connectedness. Notably, high cost choice points occur in environments that have adjacent regions of highly and poorly connected cells where planning becomes crucial. Interestingly, we find that complex environments resemble terrestrial habitats, supporting our hypothesis that a habitat shift advantaged planning over habit and is therefore likely to have been a key factor in the evolution of planning circuitry in select terrestrial vertebrates.<br>

# July 1, 2019, 5 PM
**Neuronal multi-omics, Dr. Yue Yang**<br>
Neuronal-activity-dependent transcription couples sensory experience to adaptive responses of the brain including learning and memory. I will discuss how sensory experience remodels chromatin architecture in the adult brain in vivo to orchestrate activity-dependent transcription and learning and memory. We use large-scale chromatin profiling, CRISPR genetics, and systems neuroscience approaches to study the role of long-distance enhancer-promoter and compartment interactions in the control of neural circuits in the brain.

# May 20, 2019, 5PM
**Applications of Information Theory in Early Sensory Systems, Dr. Greg Schwartz**<br>
Dr. Schwartz will talk about the basics of information theory from the first principles, including the history of the field. He will then provide some examples of the use of information theory in neuroscience. He will later discuss selected portions of the attached paper.

# May 13, 2019, 4PM
**Decoding Neural Oscillations, Nathan Whitmore**<br>
We will talk about how the Computational Neuroscience Club built a mind-controlled video game and how computational analysis can help us understand the brain's electrical signals.

# May 6, 2019, 5PM
**Introduction to Multivoxel Pattern Analysis (MVPA), Dr. Thorsten Kahnt**<br>
This session will provide a brief conceptual introduction to multivoxel pattern analysis (MVPA) for fMRI and discuss why, depending on the research question, using these methods can be advantageous. The focus will be on classification and regression approaches rather than representational similarity. We will discuss the general approach and step through one example. We will conclude by discussing potential mechanisms underlying multivoxel patterns and caveats of the method.

# April 22, 2019, 5 PM
**White Matter Tractography in DTI, Shiloh Cooper**<br>
Diffusion MRI is sensitive to water movements through tissues, and is used to characterize tissue structure based on the restriction of water movement. One example is white matter tractography, where the water pathways are assumed to travel in the same directions as bundles of axons traveling between cortical regions of interest. How can we create an accurate tractography algorithm (i.e., one that does not "jump" from one white matter tract to another when they cross within a voxel) without using anatomical priors and biasing the algorithm?

# April 8, 2019, 5 PM
**Single-Cell RNA Sequencing: A neuro-centric introduction, Rogan Grant**<br>
In the last decade, high-throughput sequencing technologies have enabled researchers to dissect transcriptional responses to a wide array of stimuli. Unfortunately, “bulk” tissue approaches have often been of limited utility to the neuroscience community, owing to the vast cellular complexity of the mammalian brain. The advent of single-cell RNA sequencing (scRNA-seq) therefore has the potential to enable researchers to study transcriptional remodeling within the brain across a wide variety of experimental manipulations. In this interactive lecture, we will process a publicly available single scRNA-seq dataset from raw sequencing data to the assignment and exploration of cell-type clusters. Particular emphasis will be given to the strengths and weaknesses of the technique, as well as current methods for data processing, visualization, and differential expression analysis. Attendees are strongly encouraged to bring a computer to the lecture.

# March 11, 2019, 5 PM
**Big data and data science in neuroscience, Torben Noto**<br>
"Big Data" and "Data science" are concepts that have gained considerable attention over the last few years but have unclear definitions and are not traditionally taught in academic circles. In the first part of this talk I will first introduce some basic ideas in data science in a way that will hopefully separate its rhetoric from its value and potentially think about analyzing your data in new ways. Data science is becoming especially important as publicly available neuroscience datasets like the Allen Brain Atlas rapidly emerge. These datasets provide many exciting opportunities in research. However, using these datasets can be tricky. In the second part of this talk we'll discuss some of these datasets as well as the basics of using them. In the final part of the talk, we'll use what we've learned to discuss the [methods paper behind Neurosynth](https://www.nature.com/articles/nmeth.1635), a metascientific fMRI mapping tool.

# February 25, 2019 5PM
**Recurrent Neural Networks for Continuous Neural Decoding, Maite Azcorra Sedano**<br>
In this session we will discuss the challenges of predicting continuous variables from neural activity. We will introduce different methods to approach this problem and focus on LSTM (Long-Short Term Memory) recurrent neural networks and their benefits. We will also discuss important things to keep in mind when implementing such networks, such as cross validation, data preprocessing, optimization of hyperparameters etc. We will finally over some real life examples of how to (and not to) implement LSTMs to predict mouse behavior from GCaMP neuronal activity.<br>
Prerequisites: Interest in Machine Learning!<br>

# February 11, 2019, 5PM
**Neural encoding and decoding in MATLAB, Vivek Sagar** <br>
For the first half of the discussion, we will briefly discuss principles of neural encoding and decoding based on the following [paper](https://www.sciencedirect.com/science/article/pii/S0079612306650310). Second half of the discussion will be based on practically implementing these models in MATLAB. We will construct an encoder (and decoder) for mapping spatial position to neural activity (and vice-versa) in MECIII grid cells in freely behaving rats. For this we will use the following [dataset](https://archive.norstore.no/pages/public/datasetDetail.jsf?id=8F6BE356-3277-475C-87B1-C7A977632DA7) and [codes](https://github.com/viveksgr/NUCNC_demo/tree/master/Scripts) for analysis.
Prerequisites: basic probability theory, coding in MATLAB
